---
layout: post
title:  "Gradient Boosting Forest: a Two-Stage Ensemble Method Enabling Federated Learning of GBDTs"
date:   2021-11-01 21:21:53 +00:00
image: /images/thumbnail_02.png
categories: ICONIP
venue: "International Conference on Neural Information Processing (ICONIP2021)"
author: "Feng Wang"
authors: "<strong>Feng Wang</strong>, Jinxiang Ou, Hairong Lv"
pdf: https://link.springer.com/chapter/10.1007/978-3-030-92270-2_7
---
Gradient Boosting Decision Trees (GBDTs), which train a set of decision trees in sequence with a gradient boosting strategy to fit the features of training data, has become very popular in recent years due to its strong capability in dealing with machine learning tasks. In many well-known machine learning competitions, GBDT even outperforms very complicate deep neural networks. Nevertheless, training such tree-based models requires accessing the whole dataset to find the split points on the features, which makes distributed training of GBDT models difficult. Particularly, in Federated Learning (FL), where training data is decentralized distributed and cannot be shared considering the privacy and security, training GBDT becomes challenging. To address this issue, in this paper, we propose a new tree-boosting method, named Gradient Boosting Forest (GBF), where the single decision tree in each gradient boosting round of GBDT is replaced by a set of trees trained from different subsets of the training data (referred to as a forest), which enables training GBDT in Federated Learning scenarios. We empirically prove that GBF outperforms the existing GBDT methods in both centralized (GBF-Cen) and federated (GBF-Fed) cases.
